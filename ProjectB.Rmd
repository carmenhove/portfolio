---
title: "Machine Learning"
output: html_document
---

```{r , include=FALSE}
load("Portfolio.RData")
inst = lapply(pkgs, library, character.only = TRUE) 
```

<font size="5"> A machine learning project using logistic regression, Naïve Bayes, and *k*-Nearest neighbors to predict precipitation in Seattle </font size="5">

![](SeattleSkyline.jpeg){width=75%}

### Data importing and cleaning
<font size="4"> The complete **R code** for this project lives [here](https://github.com/carmenhove/portfolio/blob/main/ProjectB.R){target="_blank"}. </font size="4">

<font size="3"> I used weather data collected by [National Weather Service](https://w1.weather.gov/climate/xmacis.php?wfo=sew){target="_blank"} for every December day in Seattle from 1990 to 2021. These data included date, max/min/average temperature, average temperature departure from "normal" (departure), heating degree days (HDD), cooling degree days (CDD), and inches precipitation.

```{r}
str(PBdf1)
```

All variables had values with leading and/or trailing white spaces that couldn't be removed using a simple gsub() function. Closer examination showed that this was because most of the variables were in utf8 format, so I created a simple function to fix these erroneously formatted values.

```{r}
utf8::utf8_print(unique(PBdf1$Ave.Temp), utf8 = FALSE)
replace.values <- function(x) x <- iconv(x, "latin1", "ASCII", sub="")
```

In addition to replacing the utf8 formatting, I corrected formatting inconsistencies in the Date variable and created a binary precipitation variable for classification analysis called PrecipB (0 = NO precipitation, 1 = YES precipitation).
```{r}
PBdf2 <- PBdf1 %>% 
  #Remove "T" and "M" values for precipitation, as there is no public documentation to indicate what these values mean (and there are only a few instances in the dataset)
  filter(!grepl("T",Precipitation, fixed = T),
         !grepl("M",Precipitation, fixed = T)) %>% 
  #Replace all utf8 values
  mutate_all(replace.values) %>% 
  #Fix dates, which were coded differently for 2021 versus all previous years
  mutate(Date = case_when(grepl("/",Date,fixed = T) ~ 
                            as.Date(Date, format= "%m/%d/%y"),
                          grepl("-",Date,fixed = T) ~ 
                            as.Date(Date, format= "%Y-%m-%d"))) %>% 
  #Separate out Date into month, day, and year
  mutate(Year = lubridate::year(Date), 
         Month = lubridate::month(Date), 
         Day = lubridate::day(Date)) %>% 
  #Make sure relevant variables are in numeric format
  mutate(across(c(Max.Temp:Precipitation,Day,Month,Departure,Year), as.numeric)) %>%
  #Create binary presence/absence variables for precipitation
  mutate(PrecipB = case_when(Precipitation > 0 ~ 1,
                             Precipitation ==0 ~ 0)) %>% 
  #Select out CDD, which only has a value of zero (makes sense, given likelihood of having to cool your house during December in Seattle)
  select(PrecipB,ends_with("Temp"),HDD,Departure, Day,Year)
```

After cleaning, the dataset was ready for splitting. 
```{r}
str(PBdf2)
```

### Splitting data 

I separated the initial dataset into training and testing datasets using a 3:1 split ratio. 
```{r}
set.seed(1234)
sample_set <- sample(nrow(PBdf2),round(nrow(PBdf2)*0.75), replace = F)
training <- PBdf2[sample_set,]
testing <- PBdf2[-sample_set,]
```

### Class imbalance 
As shown below, there was a class imbalance in PrecipB (more precipitation days than non-precipitation days), so I used Synthetic Minority Oversampling Technique (SMOTE) to balance the training dataset.
```{r}
round(prop.table(table(select(PBdf2, PrecipB),exclude = NULL)),4)*100
smote <- SMOTE(training, training$PrecipB)
training.adj <- smote$data %>% select(-class)
round(prop.table(table(select(training.adj, PrecipB),exclude = NULL)),4)*100
```

### Multicollinearity
Before model-building, I examined the potential for multicollinearity between variables. Given the inherent relationship between minimum, maximum, and average temperature, HDD, CDD, and Departure I expected to find substantial multicollinearity between these variables but was unsure which would be the best proxy for current weather. As shown below, Departure was strongly correlated with all the other weather-related variables, so I decided to use Departure as my proxy for weather/temp (with Day and Year as additional co-variates). 
```{r, fig.height = 3, fig.width = 6, fig.align = "left",echo=FALSE}
PBdf2 %>% keep(is.numeric) %>% cor() %>% corrplot(type = "upper")
```

### Logistic regression
Since my aim was to determine how well Departure predicted presence/absence of precipitation, Precip was my outcome variable and Departure was my primary explanatory variable with Day and Year included as predictors (shown under "class" in the model summary below). 

```{r}
summary(logitm1)
```

A quick check of the variance inflation factors (VIF) for each predictor variable indicated there was no substantial multicollinearity between variables (i.e, no VIF values about 5).
```{r}
vif(logitm1)
```

To get the estimated change in odds rather than log-odds, I exponentiated the raw model coefficients. As shown below, assuming other co-variates are held constant, for one unit increase in Departure the odds of precipitation increase by a factor of ~1.25. 
```{r}
exp(coef(logitm1)[c("Departure","Year","Day")])
```

Since everything about my initial training model checked out, I then used the training model to predict precipitation values in the testing dataset. 
```{r}
logitpred1 <- predict(logitm1, testing, type = 'response')
summary(logitpred1)
```

The logistic regression model had a predictive accuracy of **77.06%**. 
```{r, echo = FALSE}
sum(diag(logit.ptable))/nrow(testing)
```

### Naïve Bayes
Another method used in classification problems is Naïve Bayes, which can be used when the response variable has 2 or more categories. Here, I used Naïve Bayes to estimate the probability of precipitation and compared the predictive accuracy to that of the logistic regression model using the same testing and training datasets. 
```{r}
bayes.mod1 <- naiveBayes(PrecipB ~ Day + Year + Departure,
                         data = training.adj, laplace = 1)
```

As shown below, the predictive accuracy for the Naïve Bayes model was **78.35%** - slightly above the predictive accuracy of the logistic regression model.
```{r}
bayes.pred1 <- predict(bayes.mod1, testing, type = "class")
bayes.pred1.table <- table(testing$PrecipB, bayes.pred1)
bayes.pred1.table
sum(diag(bayes.pred1.table))/nrow(testing)
```

### *k*-Nearest Neighbors
After running the logistic regression and Naïve Bayes models, I also compared the predictive accuracy of both of these methods to *k*-Nearest Neighbors. Since *k*-Nearest Neighbors uses Euclidean Distance to generate classification predictions, the variables in the "model" needed to be normalized, which I did by z-scoring all predictor variables. I also used a variety of k-values to determine the effects on predictive accuracy, as shown below. Overall, the predictive accuracy was extremely high (ranging from 95.24% to 99.57% depending on the specified k-value). 
```{r, fig.height = 3, fig.width = 4, fig.align = "left",echo=FALSE}
ggplot(knn.df, aes(x = Kvalue, y = Value))+
  geom_point()+
  geom_line()+
  ylab("Predictive accuracy (%)")+
  xlab("K-value")
```

#### Cross-Validation
1. k-Fold cross-validation
2. Leave-one-out cross validation
3. Random cross-validation

#### Bootstrap Sampling
1. 

### Beyond predictive accuracy
1. Kappa
2. Precision and recall
3. Sensitivity and specificity 

###  Visualizing how the model performs
1. Receiver Operating Characteristic Curve (ROC)
2. Under the area curve (AUC)
3. 

### Conclusion
By comparing logistic regression, Naïve Bayes, and *k*-Nearest Neighbors, it's clear that *k*-Nearest Neighbors provides the best predictions on whether or not it will precipitate in Seattle on any given day in December. This is likely influenced by the relatively small sample size and low dimensionality of this particular dataset, which allows for fast computation using this lazy-learning method.  

![](rain2.jpeg){width=75%}

