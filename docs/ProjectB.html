<!DOCTYPE html>

<html>

<head>

<meta charset="utf-8" />
<meta name="generator" content="pandoc" />
<meta http-equiv="X-UA-Compatible" content="IE=EDGE" />




<title>Machine Learning</title>

<script src="site_libs/header-attrs-2.13/header-attrs.js"></script>
<script src="site_libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/bootstrap.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<style>h1 {font-size: 34px;}
       h1.title {font-size: 38px;}
       h2 {font-size: 30px;}
       h3 {font-size: 24px;}
       h4 {font-size: 18px;}
       h5 {font-size: 16px;}
       h6 {font-size: 12px;}
       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}
       pre:not([class]) { background-color: white }</style>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-9.12.0/default.css" rel="stylesheet" />
<script src="site_libs/highlightjs-9.12.0/highlight.js"></script>

<style type="text/css">
  code{white-space: pre-wrap;}
  span.smallcaps{font-variant: small-caps;}
  span.underline{text-decoration: underline;}
  div.column{display: inline-block; vertical-align: top; width: 50%;}
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
  ul.task-list{list-style: none;}
    </style>

<style type="text/css">code{white-space: pre;}</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>









<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
img {
  max-width:100%;
}
.tabbed-pane {
  padding-top: 12px;
}
.html-widget {
  margin-bottom: 20px;
}
button.code-folding-btn:focus {
  outline: none;
}
summary {
  display: list-item;
}
details > summary > p:only-child {
  display: inline;
}
pre code {
  padding: 0;
}
</style>


<style type="text/css">
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu>.dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
  border-radius: 0 6px 6px 6px;
}
.dropdown-submenu:hover>.dropdown-menu {
  display: block;
}
.dropdown-submenu>a:after {
  display: block;
  content: " ";
  float: right;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
  border-width: 5px 0 5px 5px;
  border-left-color: #cccccc;
  margin-top: 5px;
  margin-right: -10px;
}
.dropdown-submenu:hover>a:after {
  border-left-color: #adb5bd;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left>.dropdown-menu {
  left: -100%;
  margin-left: 10px;
  border-radius: 6px 0 6px 6px;
}
</style>

<script type="text/javascript">
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.tab('show');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');

  // Navbar adjustments
  var navHeight = $(".navbar").first().height() + 15;
  var style = document.createElement('style');
  var pt = "padding-top: " + navHeight + "px; ";
  var mt = "margin-top: -" + navHeight + "px; ";
  var css = "";
  // offset scroll position for anchor links (for fixed navbar)
  for (var i = 1; i <= 6; i++) {
    css += ".section h" + i + "{ " + pt + mt + "}\n";
  }
  style.innerHTML = "body {" + pt + "padding-bottom: 40px; }\n" + css;
  document.head.appendChild(style);
});
</script>

<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
  background-color: transparent;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<!-- code folding -->




</head>

<body>


<div class="container-fluid main-container">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-bs-toggle="collapse" data-target="#navbar" data-bs-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">Portfolio</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">About Me</a>
</li>
<li>
  <a href="Projects.html">Projects</a>
</li>
<li>
  <a href="Statistics.html">Statistics/ML</a>
</li>
<li>
  <a href="Bioanalytics.html">Bioanalytics</a>
</li>
<li>
  <a href="Publications.html">Publications</a>
</li>
<li>
  <a href="CV.html">CV</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div id="header">



<h1 class="title toc-ignore">Machine Learning</h1>

</div>


<p><font size="5"> A machine learning project using logistic regression,
Naïve Bayes, and <em>k</em>-Nearest neighbors to predict precipitation
in Seattle </font size="5"></p>
<p><img src="SeattleSkyline.jpeg" style="width:75.0%" /></p>
<div id="data-importing-and-cleaning" class="section level3">
<h3>Data importing and cleaning</h3>
<p><font size="4"> The complete <strong>R code</strong> for this project
lives <a
href="https://github.com/carmenhove/portfolio/blob/main/ProjectB.R"
target="_blank">here</a>. </font size="4"></p>
<p><font size="3"> I used weather data collected by <a
href="https://w1.weather.gov/climate/xmacis.php?wfo=sew"
target="_blank">National Weather Service</a> for every December day in
Seattle from 1990 to 2021. These data included date, max/min/average
temperature, average temperature departure from “normal” (departure),
heating degree days (HDD), cooling degree days (CDD), and inches
precipitation.</p>
<pre class="r"><code>str(PBdf1)</code></pre>
<pre><code>## &#39;data.frame&#39;:    989 obs. of  8 variables:
##  $ Date         : chr  &quot;12/1/21&quot; &quot;12/2/21&quot; &quot;12/3/21&quot; &quot;12/4/21&quot; ...
##  $ Max.Temp     : chr  &quot;58&quot; &quot;51&quot; &quot;46&quot; &quot;43&quot; ...
##  $ Min.Temp     : chr  &quot;50&quot; &quot;39&quot; &quot;32&quot; &quot;37&quot; ...
##  $ Ave.Temp     : chr  &quot;54&quot; &quot;45&quot; &quot;39&quot; &quot;40&quot; ...
##  $ Departure    : chr  &quot;13.8&quot; &quot;5&quot; &quot;-0.9&quot; &quot;0.3&quot; ...
##  $ HDD          : chr  &quot;11&quot; &quot;20&quot; &quot;26&quot; &quot;25&quot; ...
##  $ CDD          : chr  &quot;0&quot; &quot;0&quot; &quot;0&quot; &quot;0&quot; ...
##  $ Precipitation: chr  &quot;0&quot; &quot;0.04&quot; &quot;0&quot; &quot;0.64&quot; ...</code></pre>
<p>All variables had values with leading and/or trailing white spaces
that couldn’t be removed using a simple gsub() function. Closer
examination showed that this was because most of the variables were in
utf8 format, so I created a simple function to fix these erroneously
formatted values.</p>
<pre class="r"><code>utf8::utf8_print(unique(PBdf1$Ave.Temp), utf8 = FALSE)</code></pre>
<pre><code>##   [1] &quot;54&quot;         &quot;45&quot;         &quot;39&quot;         &quot;40&quot;         &quot;38&quot;        
##   [6] &quot;41&quot;         &quot;47&quot;         &quot;41.5&quot;       &quot;38.5&quot;       &quot;39.5&quot;      
##  [11] &quot;42.5&quot;       &quot;36&quot;         &quot;37&quot;         &quot;35.5&quot;       &quot;40.5&quot;      
##  [16] &quot;43&quot;         &quot;35&quot;         &quot;29.5&quot;       &quot;22.5&quot;       &quot;28.5&quot;      
##  [21] &quot;25.5&quot;       &quot;34&quot;         &quot;22&quot;         &quot;37.0\u00a0&quot; &quot;38.5\u00a0&quot;
##  [26] &quot;35.5\u00a0&quot; &quot;39.5\u00a0&quot; &quot;40.5\u00a0&quot; &quot;44.5\u00a0&quot; &quot;48.0\u00a0&quot;
##  [31] &quot;50.0\u00a0&quot; &quot;42.5\u00a0&quot; &quot;36.5\u00a0&quot; &quot;40.0\u00a0&quot; &quot;44.0\u00a0&quot;
##  [36] &quot;45.0\u00a0&quot; &quot;46.5\u00a0&quot; &quot;47.0\u00a0&quot; &quot;46.0\u00a0&quot; &quot;45.5\u00a0&quot;
##  [41] &quot;49.5\u00a0&quot; &quot;33.0\u00a0&quot; &quot;32.5\u00a0&quot; &quot;37.5\u00a0&quot; &quot;36.0\u00a0&quot;
##  [46] &quot;47.5\u00a0&quot; &quot;43.5\u00a0&quot; &quot;42.0\u00a0&quot; &quot;41.5\u00a0&quot; &quot;38.0\u00a0&quot;
##  [51] &quot;39.0\u00a0&quot; &quot;51.5\u00a0&quot; &quot;35.0\u00a0&quot; &quot;33.5\u00a0&quot; &quot;30.5\u00a0&quot;
##  [56] &quot;27.0\u00a0&quot; &quot;41.0\u00a0&quot; &quot;34.0\u00a0&quot; &quot;32.0\u00a0&quot; &quot;34.5\u00a0&quot;
##  [61] &quot;31.0\u00a0&quot; &quot;31.5\u00a0&quot; &quot;29.0\u00a0&quot; &quot;30.0\u00a0&quot; &quot;49.0\u00a0&quot;
##  [66] &quot;43.0\u00a0&quot; &quot;26.5\u00a0&quot; &quot;24.5\u00a0&quot; &quot;50.5\u00a0&quot; &quot;55.0\u00a0&quot;
##  [71] &quot;27.5\u00a0&quot; &quot;48.5\u00a0&quot; &quot;53.0\u00a0&quot; &quot;55.5\u00a0&quot; &quot;51.0\u00a0&quot;
##  [76] &quot;29.5\u00a0&quot; &quot;28.5\u00a0&quot; &quot;22.5\u00a0&quot; &quot;21.5\u00a0&quot; &quot;20.5\u00a0&quot;
##  [81] &quot;23.0\u00a0&quot; &quot;28.0\u00a0&quot; &quot;54.0\u00a0&quot; &quot;25.0\u00a0&quot; &quot;21.0\u00a0&quot;
##  [86] &quot;18.5\u00a0&quot; &quot;19.0\u00a0&quot; &quot;19.5\u00a0&quot; &quot;25.5\u00a0&quot; &quot;26.0\u00a0&quot;
##  [91] &quot;22.0\u00a0&quot; &quot;17.0\u00a0&quot; &quot;53.5\u00a0&quot; &quot;56.0\u00a0&quot; &quot;52.5\u00a0&quot;
##  [96] &quot;17.5\u00a0&quot; &quot;16.0\u00a0&quot; &quot;12.0\u00a0&quot; &quot;14.5\u00a0&quot; &quot;M\u00a0&quot;   
## [101] &quot;20.0\u00a0&quot; &quot;12.5\u00a0&quot; &quot;11.0\u00a0&quot; &quot;16.5\u00a0&quot; &quot;14.0\u00a0&quot;</code></pre>
<pre class="r"><code>replace.values &lt;- function(x) x &lt;- iconv(x, &quot;latin1&quot;, &quot;ASCII&quot;, sub=&quot;&quot;)</code></pre>
<p>In addition to replacing the utf8 formatting, I corrected formatting
inconsistencies in the Date variable and created a binary precipitation
variable for classification analysis called PrecipB (0 = NO
precipitation, 1 = YES precipitation).</p>
<pre class="r"><code>PBdf2 &lt;- PBdf1 %&gt;% 
  #Remove &quot;T&quot; and &quot;M&quot; values for precipitation, as there is no public documentation to indicate what these values mean (and there are only a few instances in the dataset)
  filter(!grepl(&quot;T&quot;,Precipitation, fixed = T),
         !grepl(&quot;M&quot;,Precipitation, fixed = T)) %&gt;% 
  #Replace all utf8 values
  mutate_all(replace.values) %&gt;% 
  #Fix dates, which were coded differently for 2021 versus all previous years
  mutate(Date = case_when(grepl(&quot;/&quot;,Date,fixed = T) ~ 
                            as.Date(Date, format= &quot;%m/%d/%y&quot;),
                          grepl(&quot;-&quot;,Date,fixed = T) ~ 
                            as.Date(Date, format= &quot;%Y-%m-%d&quot;))) %&gt;% 
  #Separate out Date into month, day, and year
  mutate(Year = lubridate::year(Date), 
         Month = lubridate::month(Date), 
         Day = lubridate::day(Date)) %&gt;% 
  #Make sure relevant variables are in numeric format
  mutate(across(c(Max.Temp:Precipitation,Day,Month,Departure,Year), as.numeric)) %&gt;%
  #Create binary presence/absence variables for precipitation
  mutate(PrecipB = case_when(Precipitation &gt; 0 ~ 1,
                             Precipitation ==0 ~ 0)) %&gt;% 
  #Select out CDD, which only has a value of zero (makes sense, given likelihood of having to cool your house during December in Seattle)
  select(PrecipB,ends_with(&quot;Temp&quot;),HDD,Departure, Day,Year)</code></pre>
<p>After cleaning, the dataset was ready for splitting.</p>
<pre class="r"><code>str(PBdf2)</code></pre>
<pre><code>## &#39;data.frame&#39;:    923 obs. of  8 variables:
##  $ PrecipB  : num  0 1 0 1 1 1 1 1 1 1 ...
##  $ Max.Temp : num  58 51 46 43 43 46 50 48 43 44 ...
##  $ Min.Temp : num  50 39 32 37 33 36 44 35 34 35 ...
##  $ Ave.Temp : num  54 45 39 40 38 41 47 41.5 38.5 39.5 ...
##  $ HDD      : num  11 20 26 25 27 24 18 23 26 25 ...
##  $ Departure: num  13.8 5 -0.9 0.3 -1.6 1.6 7.7 2.3 -0.6 0.5 ...
##  $ Day      : num  1 2 3 4 5 6 7 8 9 10 ...
##  $ Year     : num  2021 2021 2021 2021 2021 ...</code></pre>
</div>
<div id="splitting-data" class="section level3">
<h3>Splitting data</h3>
<p>I separated the initial dataset into training and testing datasets
using a 3:1 split ratio.</p>
<pre class="r"><code>set.seed(1234)
sample_set &lt;- sample(nrow(PBdf2),round(nrow(PBdf2)*0.75), replace = F)
training &lt;- PBdf2[sample_set,]
testing &lt;- PBdf2[-sample_set,]</code></pre>
</div>
<div id="class-imbalance" class="section level3">
<h3>Class imbalance</h3>
<p>As shown below, there was a class imbalance in PrecipB (more
precipitation days than non-precipitation days), so I used Synthetic
Minority Oversampling Technique (SMOTE) to balance the training
dataset.</p>
<pre class="r"><code>round(prop.table(table(select(PBdf2, PrecipB),exclude = NULL)),4)*100</code></pre>
<pre><code>## 
##     0     1 
## 29.58 70.42</code></pre>
<pre class="r"><code>smote &lt;- SMOTE(training, training$PrecipB)
training.adj &lt;- smote$data %&gt;% select(-class)
round(prop.table(table(select(training.adj, PrecipB),exclude = NULL)),4)*100</code></pre>
<pre><code>## 
##     0     1 
## 46.39 53.61</code></pre>
</div>
<div id="multicollinearity" class="section level3">
<h3>Multicollinearity</h3>
<p>Before model-building, I examined the potential for multicollinearity
between variables. Given the inherent relationship between minimum,
maximum, and average temperature, HDD, CDD, and Departure I expected to
find substantial multicollinearity between these variables but was
unsure which would be the best proxy for current weather. As shown
below, Departure was strongly correlated with all the other
weather-related variables, so I decided to use Departure as my proxy for
weather/temp (with Day and Year as additional co-variates).
<img src="ProjectB_files/figure-html/unnamed-chunk-8-1.png" width="576" style="display: block; margin: auto auto auto 0;" /></p>
</div>
<div id="logistic-regression" class="section level3">
<h3>Logistic regression</h3>
<p>Since my aim was to determine how well Departure predicted
presence/absence of precipitation, Precip was my outcome variable and
Departure was my primary explanatory variable with Day and Year included
as predictors (shown under “class” in the model summary below).</p>
<pre class="r"><code>summary(logitm1)</code></pre>
<pre><code>## 
## Call:
## glm(formula = PrecipB ~ Departure + Day + Year, family = binomial, 
##     data = training.adj)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.4025  -0.9285   0.3707   0.8978   2.9536  
## 
## Coefficients:
##              Estimate Std. Error z value Pr(&gt;|z|)    
## (Intercept) 22.799989  17.682502   1.289  0.19726    
## Departure    0.220891   0.017465  12.648  &lt; 2e-16 ***
## Day          0.027820   0.008818   3.155  0.00161 ** 
## Year        -0.011404   0.008817  -1.293  0.19589    
## ---
## Signif. codes:  0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1244.36  on 900  degrees of freedom
## Residual deviance:  977.25  on 897  degrees of freedom
## AIC: 985.25
## 
## Number of Fisher Scoring iterations: 5</code></pre>
<p>A quick check of the variance inflation factors (VIF) for each
predictor variable indicated there was no substantial multicollinearity
between variables (i.e, no VIF values about 5).</p>
<pre class="r"><code>vif(logitm1)</code></pre>
<pre><code>## Departure       Day      Year 
##  1.010410  1.006763  1.003814</code></pre>
<p>To get the estimated change in odds rather than log-odds, I
exponentiated the raw model coefficients. As shown below, assuming other
co-variates are held constant, for one unit increase in Departure the
odds of precipitation increase by a factor of ~1.25.</p>
<pre class="r"><code>exp(coef(logitm1)[c(&quot;Departure&quot;,&quot;Year&quot;,&quot;Day&quot;)])</code></pre>
<pre><code>## Departure      Year       Day 
## 1.2471880 0.9886608 1.0282109</code></pre>
<p>Since everything about my initial training model checked out, I then
used the training model to predict precipitation values in the testing
dataset.</p>
<pre class="r"><code>logitpred1 &lt;- predict(logitm1, testing, type = &#39;response&#39;)
summary(logitpred1)</code></pre>
<pre><code>##     Min.  1st Qu.   Median     Mean  3rd Qu.     Max. 
## 0.009712 0.405948 0.618488 0.576120 0.761160 0.967987</code></pre>
<p>The logistic regression model had a predictive accuracy of
<strong>77.06%</strong>.</p>
<pre><code>## [1] 0.7705628</code></pre>
</div>
<div id="naïve-bayes" class="section level3">
<h3>Naïve Bayes</h3>
<p>Another method used in classification problems is Naïve Bayes, which
can be used when the response variable has 2 or more categories. Here, I
used Naïve Bayes to estimate the probability of precipitation and
compared the predictive accuracy to that of the logistic regression
model using the same testing and training datasets.</p>
<pre class="r"><code>bayes.mod1 &lt;- naiveBayes(PrecipB ~ Day + Year + Departure,
                         data = training.adj, laplace = 1)</code></pre>
<p>As shown below, the predictive accuracy for the Naïve Bayes model was
<strong>78.35%</strong> - slightly above the predictive accuracy of the
logistic regression model.</p>
<pre class="r"><code>bayes.pred1 &lt;- predict(bayes.mod1, testing, type = &quot;class&quot;)
bayes.pred1.table &lt;- table(testing$PrecipB, bayes.pred1)
bayes.pred1.table</code></pre>
<pre><code>##    bayes.pred1
##       0   1
##   0  44  20
##   1  30 137</code></pre>
<pre class="r"><code>sum(diag(bayes.pred1.table))/nrow(testing)</code></pre>
<pre><code>## [1] 0.7835498</code></pre>
</div>
<div id="k-nearest-neighbors" class="section level3">
<h3><em>k</em>-Nearest Neighbors</h3>
<p>After running the logistic regression and Naïve Bayes models, I also
compared the predictive accuracy of both of these methods to
<em>k</em>-Nearest Neighbors. Since <em>k</em>-Nearest Neighbors uses
Euclidean Distance to generate classification predictions, the variables
in the “model” needed to be normalized, which I did by z-scoring all
predictor variables. I also used a variety of k-values to determine the
effects on predictive accuracy, as shown below. Overall, the predictive
accuracy was extremely high (ranging from 95.24% to 99.57% depending on
the specified k-value).
<img src="ProjectB_files/figure-html/unnamed-chunk-16-1.png" width="384" style="display: block; margin: auto auto auto 0;" /></p>
</div>
<div id="conclusion" class="section level3">
<h3>Conclusion</h3>
<p>By comparing logistic regression, Naïve Bayes, and <em>k</em>-Nearest
Neighbors, it’s clear that <em>k</em>-Nearest Neighbors provides the
best predictions on whether or not it will precipitate in Seattle on any
given day in December. This is likely influenced by the relatively small
sample size and low dimensionality of this particular dataset, which
allows for fast computation using this lazy-learning method.</p>
<p><img src="rain2.jpeg" style="width:75.0%" /></p>
</div>




</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- tabsets -->

<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open');
  });
});
</script>

<!-- code folding -->


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
